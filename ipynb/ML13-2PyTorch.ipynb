{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "## 학습목표\n",
    "\n",
    "- 신경망 구현을 위한 패키지\n",
    "- PyTorch 란?\n",
    "- PyTorch 환경 구축\n",
    "- PyTorch 를 이용한 MNIST 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망 구현을 위한 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 강의에서는 PyTorch 를 소개하겠습니다. PyTorch 를 사용한다면 간단하게 신경망을 구성할 수 있습니다. 데이터를 읽어드린 다음, 신경망을 구성하고 학습시켜 결과를 예측하는 것을 보여드리겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch 란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pytorch.PNG\" width=\"500\">\n",
    "<br><center>그림 1: Pytorch [출처](https://pytorch.org/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lua 언어로 개발된 Torch라는 딥러닝 라이브러리가 있었습니다. 머신러닝 라이브러리이자 Scientific Computing 프레임 워크 였습니다. 하지만, Lua 기반이라 다른 라이브러리에 비해 업데이트 속도도 느리고 사용자도 적었습니다. 그런데 이 Torch 를 Facebook에서 Python API로 개발하였습니다. 그러면서 폭발적인 인기를 얻게 되었고, 오늘 여러분들께 소개해 드리게 된 딥러닝 라이브러리 PyTorch입니다.\n",
    "\n",
    "디버깅이 쉬운 직관적인 코드로 구성되어있다.\n",
    "모델 그래프가 고정 상태가 아니기 때문에 언제든지 데이터에 따라 모델 조정 작업이 가능하다. (모델을 feed 하면서 정의하기 때문에 언제든 수정이 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch 환경 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 를 설치하기에 앞서 backend 엔진으로 사용할 TensorFlow 혹은 Theano, CNTK 중에 하나를 골라서 설치해야 합니다. 이번 강의에서 우리는 tensorflow 를 설치할 것입니다. \n",
    "\n",
    "지금까지의 경험으로보면, 가장 쉽게 Keras 를 설치하는 방법은 다음과 같습니다. Anaconda 를 우선 설치하고, Conda 명령어로 tensorflow 를 설치한 후에 Keras 를 설치하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Anaconda 설치\n",
    "\n",
    "Anaconda3 를 설치하세요. 다음 링크를 통해 설치하시면 됩니다: [링크](https://www.continuum.io/downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensorflow 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OS X or Linux\n",
    "\n",
    "아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.6\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "\n",
    "cmd 혹은 Anaconda 쉘에서 아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.6\n",
    "activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "Tensorflow 가 정상적으로 설치되었는지 확인하기 위해 아래의 코드를 실행해보세요. tensorflow 라이브러리를 import 할 때에 문제가 생기지 않는다면, 정상적으로 설치된 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 PyTorch 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 까지 설치했다면, Keras 설치하는건 간단합니다. 콘솔창에서 다음 명령을 실행하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install pytorch\n",
    "pip install torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 명령어를 통해 PyTorch가 정상적으로 설치되었는지 확인합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설치중에 어려움이 있다면, 아래 링크를 참고하도록 합니다.\n",
    "\n",
    "- [Anaconda 설치](https://www.continuum.io/downloads)\n",
    "- [tensorflow 설치](https://www.tensorflow.org/install/)\n",
    "- [PyTorch 설치](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch 를 이용한 MNIST 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 PyTorch 를 사용할 환경이 구축되었습니다. PyTorch 를 이용해서 MNIST 데이터를 분석해보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MNIST 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에는 datasets 라는 모듈이 있으며, 사람들이 많이 사용하는 데이터를 쉽게 사용할 수 있도록 만들어져 있습니다. MNIST 데이터를 읽어오기 위해서 아래와 같이 두 줄이면 충분합니다. torchvision.datasets 에 있는 MNIST 클래스를 호출하면 MNIST 데이터를 학습시킬 때 사용할 데이터와 테스트할 때 사용할 데이터를 얻을 수 있습니다.\n",
    "MNIST 데이터를 train과 test로 각각 얻도록 합니다.\n",
    "\n",
    "Tensorflow나 keras에서는 모델학습에 용이한 데이터처리를 위해 원핫인코딩(One-Hot-Encoding)을 사용했습니다. PyTorch에서는 이미지를 텐서로 바꿔주는 transforms를 사용할 것입니다. 이는 torchvision에서 MNIST 데이터를 불러올 때 transform 인자에 transforms 메소드의 ToTensor()를 사용하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='MNIST_data/',\n",
    "                          train=True, \n",
    "                          transform=torchvision.transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=torchvision.transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나뉘어진 데이터 세트의 형상은 아래와 같습니다. 학습할 데이터에는 60,000개의 입력값이, 테스트할 `X` 데이터에는 10,000개의 입력값이 저장되어있는 것을 볼 수 있습니다. 각각의 입력값은 28 x 28 의 크기로 0 ~ 255 범위에 있는 숫자가 저장됩니다. `y` 에는 해당 입력값의 실제 숫자 0 ~ 9가 저장됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([60000, 28, 28])\n",
      "y_train.shape: torch.Size([60000])\n",
      "X_test.shape: torch.Size([10000, 28, 28])\n",
      "y_test.shape:torch.Size([10000])\n",
      "\n",
      "X_train: minimum value=0, maximum value=255\n",
      "X_test: minimum value=0, maximum value=255\n",
      "y_train: minimum value=0, maximum value=9\n",
      "y_test: minimum value=0, maximum value=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist_train.train_data\n",
    "y_train = mnist_train.train_labels\n",
    "X_test = mnist_test.test_data\n",
    "y_test = mnist_test.test_labels\n",
    "\n",
    "print(\"X_train.shape: {}\\ny_train.shape: {}\\nX_test.shape: {}\\ny_test.shape:{}\\n\".\n",
    "      format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "print(\"X_train: minimum value={}, maximum value={}\".format(X_train.min(), X_train.max()))\n",
    "print(\"X_test: minimum value={}, maximum value={}\".format(X_test.min(), X_test.max()))\n",
    "print(\"y_train: minimum value={}, maximum value={}\".format(y_train.min(), y_train.max()))\n",
    "print(\"y_test: minimum value={}, maximum value={}\".format(y_test.min(), y_test.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 모든 feature의 값을 0 ~ 1 사이로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습을 하게되면 일반적으로 모든 feature를 0 에서 1 사이의 값으로 맞추어서 모델을 학습하곤 합니다. 여러 이유가 있겠지만, (1) 각 입력값의 특정 feature 가 큰 범위로 변할 경우 다른 feature의 영향력이 모델링 하는 단계에서 무시될 수 있기도 하며 (2) 비용함수를 계산하는 과정에서 값이 너무 크게되는 경우도 발생합니다.\n",
    "\n",
    "MNIST 데이터는 각 feature들이 왼쪽 위에서부터 오른쪽 아래까지의 pixel 값들이기 때문에, 모든 feature들의 최소값은 0이고 최대값은 255 입니다. 따라서 모든 feature들의 값을 255로 나눠주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "X_train = (X_train / 255.)\n",
    "X_test = (X_test / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 정수로 저장된 `y` 값을 one-hot 으로 바꿔주겠습니다. `torch.nn` 에 있는 `functional` 메소드를 사용해서 쉽게 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 one-hot 인코딩\n",
    "이제 `y_train` 과 `y_test` 를 one-hot 인코딩 하겠습니다. `y_train` 의 처음 5개 레이블을 `[5 0 4 1 9]` 입니다. one-hot 인코딩이 정말 간단하죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous five labels in y_train: tensor([5, 0, 4, 1, 9])\n",
      "One-hot encoded labels of y_train: \n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"previous five labels in y_train: {}\".format(y_train[:5]))\n",
    "\n",
    "y_train = F.one_hot(y_train)\n",
    "y_test = F.one_hot(y_test)\n",
    "\n",
    "print('One-hot encoded labels of y_train: \\n{}'.format(y_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 이제 본격적으로 DataLoader를 만들어주도록 하겠습니다. PyTorch는 Data를 배치 형태 또는 이미지 묶음 형태로 이용할 수 있도록 DataLoader라는 클래스를 제공합니다. \n",
    "이때 DataLoader에는 4개의 인자가 있습니다. 첫번째 인자인 dataset은 로드할 대상을 의미하며, 두번째 인자인 batch_size는 배치 크기, shuffle은 매 epoch마다 미니 배치를 셔플할 것인지의 여부, drop_last는 마지막 배치를 버릴 것인지를 의미합니다.\n",
    "\n",
    "여기서는 학습데이터로 DataLoader를 만들어주도록 하겠습니다. DataLoader를 만들기 전에 거쳐야 하는 작업이 있습니다. TensorDataset을 사용해서 학습 데이터와 레이블 데이터를 묶어서 DataLoader에 전달해주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkjs8\\AppData\\Local\\Temp/ipykernel_39540/2754724483.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\lkjs8\\AppData\\Local\\Temp/ipykernel_39540/2754724483.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
      "C:\\Users\\lkjs8\\AppData\\Local\\Temp/ipykernel_39540/2754724483.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\lkjs8\\AppData\\Local\\Temp/ipykernel_39540/2754724483.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = DataLoader(dataset=dataset_train, batch_size=32, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 신경망 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망을 구축합니다. 가장 간단한 종류의 `Linear`레이어를 사용해서 신경망을 구축하겠습니다. `Linear` 은 다른 프레임워크에서는 Dense Layer 또는 Fully Connected Layer라고 부르기도 합니다. 다수의 복수 레이어를 사용하는 개념으로 각 레이어 마다 입력 데이터읭 패턴을 학습하고, 다음 레이어는 앞에서 학습한 패턴을 기반으로 학습을 이어나갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "The model has 407,050 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = NeuralNet()\n",
    "print(model)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 신경망을 구축했으니 모델을 컴파일하면 되겠군요. 이 단계에서는 어떤 손실함수를 쓸 것인지 (loss), 어떠한 최적화기를 사용할 것인지 (optimizer), 무엇을 기준으로 학습 할 것인지 (metrics) 를 설정해줍니다. 아래의 조합은 분류를 할 때에 일반적으로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 학습시키는 단계입니다. 우선 `train()`을 이용해 모델의 상태를 학습상태로 전환합니다. optimizer의 `zero_grad()`메소드를 사용해서 Gradient를 0으로 초기화해주도록 합니다. 앞에서 만든 DataLoader를 불러와 입력값 X와 레이블 Y로 분리해줍니다. 이후 모델에 입력값 X를 입력으로하여 결과값을 얻습니다. 결과값으로 loss를 계산하고, `backward()`메소드를 통해 Gradient를 계산합니다. 그리고 `step()`메소드를 통해 parameter를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Epoch:   1| loss = 1.2809217\n",
      "|Epoch:   2| loss = 1.2039669\n",
      "|Epoch:   3| loss = 1.1873105\n",
      "|Epoch:   4| loss = 1.1783078\n",
      "|Epoch:   5| loss = 1.1721913\n",
      "|Epoch:   6| loss = 1.1679820\n",
      "|Epoch:   7| loss = 1.1644285\n",
      "|Epoch:   8| loss = 1.0782170\n",
      "|Epoch:   9| loss = 0.9403742\n",
      "|Epoch:  10| loss = 0.9352827\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() / len(data_loader)\n",
    "\n",
    "    print('|Epoch: {:3d}| loss = {:.7f}'.format(epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 분류 정확도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 테스트 세트를 예측해봅니다.\n",
    "\n",
    "`eval()`을 사용해서 model의 상태를 evaluate로 변경합니다. 그리고, 테스트 세트에 있는 입력값들의 레이블을 예측합니다.\n",
    "\n",
    "코드를 실행할 때마다 accuracy 는 차이가 있겠지만 위와 같은 인자들로 신경망을 학습시켰을 때에 88% 전후의 정확도를 보여줍니다. 즉, 0에서 9까지 적혀있는 28 x 28 사이즈의 입력값이 들어왔을 때에, 88% 정확도로 어떤 숫자인지 알아내는 신경망을 학습시킨 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6032999753952026\n"
     ]
    }
   ],
   "source": [
    "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 *28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch를 이용한 CNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 PyTorch를 이용해 CNN을 구현해보도록 하겠습니다. 먼저 데이터를 다시 불러와줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='MNIST_data/',\n",
    "                          train=True, \n",
    "                          transform=torchvision.transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=torchvision.transforms.ToTensor(),\n",
    "                         download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 모델을 만들어주도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수와 옵티마이저를 정의하고 모델을 출력해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습단계입니다. 학습을 진행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Epoch:   1| loss = 0.4284947\n",
      "|Epoch:   2| loss = 0.1545086\n",
      "|Epoch:   3| loss = 0.1190707\n",
      "|Epoch:   4| loss = 0.0997048\n",
      "|Epoch:   5| loss = 0.0887598\n",
      "|Epoch:   6| loss = 0.0818710\n",
      "|Epoch:   7| loss = 0.0757985\n",
      "|Epoch:   8| loss = 0.0717313\n",
      "|Epoch:   9| loss = 0.0672000\n",
      "|Epoch:  10| loss = 0.0634028\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "cnn.train()\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = cnn(X)\n",
    "        loss = criterion(y_hat, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item() / len(data_loader)\n",
    "\n",
    "    print('|Epoch: {:3d}| loss = {:.7f}'.format(epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 평가를 진행하겠습니다. 평가에는 학습을 진행하지 않을 것이므로 torch.no_grad()를 명시해줍니다. 평가 결과 CNN을 사용한 모델은 약 96% 정확도를 보이는 것을 확인했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9607999920845032\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.data.view(len(mnist_test), 1, 28, 28).float()\n",
    "    Y_test = mnist_test.targets\n",
    "\n",
    "    prediction = cnn(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [PyTorch Documents](https://pytorch.org/)\n",
    "- [PyTorch with MNIST Datasets](https://wikidocs.net/60324)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
